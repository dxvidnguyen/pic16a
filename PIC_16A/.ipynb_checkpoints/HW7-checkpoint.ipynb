{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "Unlike previous homework assignments, this homework is **completed as a group** and **submitted on CCLE.** In other words, it's similar to an extended Discussion Activity. \n",
    "\n",
    "> We affirm that we personally wrote the text, code, and comments in this homework assignment. \n",
    "\n",
    "> We received help from \\[\\] who gave me suggestions on \\[problem #\\].\n",
    "\n",
    "\\- \\[Lauren, Rashi, David\\] May 26, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "\n",
    "It is highly recommended that you work with your group to fully complete the previous Discussion assignments related to the project this week, as all of these are directly helpful for your project submission. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "In your project, you are required to demonstrate the use of decision trees and multinomial logistic regression classifiers. Groups of three must also demonstrate the use of one additional model. In this homework assignment, you will write a detailed report that will largely fulfill this requirement. \n",
    "\n",
    "Choose one machine learning model, other than decision trees and multinomial logistic regression (unless you are a group of 2 -- see below). Some possibilities include, but are not limited to: \n",
    "\n",
    "- Support vector classifiers (`sklearn.svm.SVC`). Complexity parameter is called `gamma`. Higher values create more complex models)\n",
    "- K-nearest-neighbor classifiers (`sklearn.neighbors.KNeighborsClassifier`). Complexity parameter is called `n_neighbors`. Higher values create less complex models. When working with KNN models, it is helpful to standardize your data columns first. For this purpose, `sklearn.preprocessing.StandardScaler` can be used. \n",
    "- Multilayer perceptron classifier, a kind of neural network (`sklearn.neural_network.MLPClassifier`). The complexity is controlled by the number of neurons and layers, as specified by the `hidden_layer_sizes` parameter -- more neurons and layers correspond to more complex models. \n",
    "\n",
    "You should consult the documentation for these models, which is available online or via `?`. \n",
    "\n",
    "\n",
    "#### Groups of 2\n",
    "\n",
    "If you are a group of 2, you are not required to use an additional model and may instead use multinomial logistic regression for this assignment. Please note this in your submission. \n",
    "\n",
    "- Multinomial logistic regression (`sklearn.linear_model.LogisticRegression`). The complexity is controlled by `C`, the inverse regularization strength. Larger values of `C` correspond to more complex models. \n",
    "\n",
    "### Instructions\n",
    "\n",
    "Replicate and expand the pipeline from Discussion 15, substituting out decision trees for your chosen classifier. In particular, you should: \n",
    "\n",
    "1. **Load** the data. \n",
    "2. **Split the data** into training and test sets. \n",
    "3. Write one or more functions to **clean and transform the data** as needed. You should add comments and function docstrings as appropriate to describe to your reader what you are doing and why. Apply your functions to the training and test sets. \n",
    "4. **Select 3 columns (features)** from the data. You are required to use at least one qualitative feature (like Island or Sex). We saw one way of doing this \"by hand\" during a previous Live Lecture, but there are many other approaches as well. You may wish to delegate one group member to do some research on the topic of \"feature selection in Python with sklearn\". \n",
    "4. **Use cross-validation** to estimate optimal model complexity (*Note*: cross-validation is also an acceptable way to perform feature selection -- you may use it above, but you are required to use it here). \n",
    "5. Having selected your features and an optimal complexity, **evaluate your model on the test set.** \n",
    "6. **Inspect** a few instances in which your trained model gave the wrong answer on the test set. Explain why your model was \"tricked\" in this case? Create and comment on a confusion matrix -- [check Monday's lecture notes](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/ML/digits.ipynb) for an example. \n",
    "7. **Plot the decision regions** for your model. These are covered in a recent live lecture, and code required to construct them is below. The horizontal and vertical axes should be the two quantitative predictor variables that you selected. Create a separate plot for each possible value of the qualitative predictor variable that you selected. For example, if you chose `Sex` as your qualitative variable, you should show two plots, with the decision regions for Female and Male penguins separately. Add appropriate axis labels and any other measures required to make your plots look professional. Add commentary to contextualize the mistakes that your model made. \n",
    "    - You are welcome to use the [code from live lecture](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/live_lectures/22-live-lecture.ipynb) for this purpose. A common pitfall when using code for manual decision regions is re-training a model (using `model.fit()`) once for each plot. You should ensure that your model is only trained once, and that its predictions are then extracted for each plot. \n",
    "\n",
    "Throughout, add helpful explanation for your reader. For example, you should explain the idea behind cross-validation; anything you are able to learn about how your model works; how to interpret the optimal complexity parameter; and why your model went wrong in certain cases.  \n",
    "\n",
    "### Additional Specifications\n",
    "\n",
    "This homework will be graded by me personally, and will be graded using a rubric very similar to that used for the final project. I'll also give you some feedback indicating how you can improve. To get the most benefit out of this feedback, you should **treat this homework as a rehearsal for the full project.** \n",
    "\n",
    "Your solution should **not** fit in a single code cell -- a good, readable, and adequately-explained solution will likely require at least one code cell for each of the eight steps above. Make sure that there is plenty of surrounding text explanation and comments. Add code cells and plenty of markdown cells as needed. You can also use section headers with `###` to organize your work. \n",
    "\n",
    "Your comments and explanation should be written to an imaginary reader who has never encountered the Palmer Penguins data set before, and knows less about machine learning than you do. Explain your steps. Why train-test split? Why cross-validation? Why decision regions? And so on. If you'd like, imagine that you are writing to a time-traveller version of you from two weeks ago. \n",
    "\n",
    "It is not necessary to submit the HW on a copy of this notebook -- you may create and submit a fresh notebook if you wold like. Regardless, \n",
    "\n",
    "#### You are required to include a Group Contributions Statement on this HW. \n",
    "\n",
    "## Your Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree, preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "url = \"https://philchodrow.github.io/PIC16A/datasets/palmer_penguins.csv\"\n",
    "penguins = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into test/train sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "penguins = penguins[['Species', 'Culmen Length (mm)', \n",
    "                     'Culmen Depth (mm)', 'Flipper Length (mm)', \n",
    "                     'Body Mass (g)', 'Sex']]\n",
    "penguins = penguins.dropna()\n",
    "# make sure that the random values that your code will generate \n",
    "# will be the same every time you run the code.\n",
    "np.random.seed(3354354524)\n",
    "\n",
    "# Since Species is the target variable...\n",
    "X = penguins.drop(['Species'], axis = 1)\n",
    "y = penguins['Species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and transform the data\n",
    "\n",
    "def clean_penguin_data(x_train, x_test, Y_train, Y_test):\n",
    "    '''\n",
    "    clean_penguin_data takes in the four train/test sets as parameters\n",
    "    and cleans the data for each and then returns the cleaned copies back\n",
    "    '''\n",
    "    xtr = x_train.copy()\n",
    "    xt = x_test.copy()\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    xtr['Sex'] = le.fit_transform(x_train['Sex'])\n",
    "    Y_train = le.fit_transform(Y_train)\n",
    "    xt['Sex'] = le.fit_transform(x_test['Sex'])\n",
    "    Y_test = le.fit_transform(Y_test)\n",
    "    \n",
    "    return (xtr, xt, Y_train, Y_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = clean_penguin_data(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flipper Length (mm) 455.7072058518755\n",
      "Culmen Depth (mm) 315.4109641320591\n",
      "Culmen Length (mm) 308.22792100679123\n",
      "Body Mass (g) 270.64662388142307\n",
      "Sex 0.3421039825847912\n"
     ]
    }
   ],
   "source": [
    "# Select 3 columns (features)\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "threshold = 5 # the number of most relevant features\n",
    "high_score_features = []\n",
    "feature_scores = f_classif(X_train, y_train)[0]\n",
    "for score, f_name in sorted(zip(feature_scores, X_train.columns), reverse=True)[:threshold]:\n",
    "      print(f_name, score)\n",
    "      high_score_features.append(f_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected Columns\n",
    "We can see that the Flipper Length, Culmen Depth, and Culmen Length are the most relevant features.\n",
    "Since it is required to have a qualitative column as well, we will choose the top two columns and one qualitative feature.\n",
    "\n",
    "So, the 3 columns that we choose are **Flipper Length, Culmen Depth, and Sex**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross-validation\n",
    "\n",
    "# First, drop the columns that aren't being used anymore\n",
    "X = penguins.drop(['Species', 'Culmen Length (mm)', 'Body Mass (g)'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "X_train, X_test, y_train, y_test = clean_penguin_data(X_train, X_test, y_train, y_test)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "best_score = 0\n",
    "\n",
    "# For 30 depths, find the highest score to find the best depth\n",
    "for d in range(1,30):\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(d, d, d), max_iter=3000)\n",
    "    cv_score = cross_val_score(clf, X_train, y_train, cv=10).mean()\n",
    "    ax.scatter(d, cv_score, color = \"black\")\n",
    "    if cv_score > best_score:\n",
    "        best_depth = d\n",
    "        best_score = cv_score\n",
    "        \n",
    "l = ax.set(title = \"Best Depth : \" + str(best_depth),\n",
    "xlabel = \"Depth\",\n",
    "ylabel = \"CV Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an MLPClassifier at best complexity, and evaluate the model on the test set\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(17, 17, 17), max_iter=3000).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and comment on a confusion matrix \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "c = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "disp = plot_confusion_matrix(clf, X_test, y_test, cmap = plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision regions\n",
    "from sklearn import tree\n",
    "def plot_regions(c, X, y):\n",
    "        \n",
    "    # for convenience, give names to the two \n",
    "    # columns of the data\n",
    "    x0 = X['Flipper Length (mm)']\n",
    "    x1 = X['Culmen Depth (mm)']\n",
    "    \n",
    "    # create a grid\n",
    "    grid_x = np.linspace(x0.min(),x0.max(),501)\n",
    "    grid_y = np.linspace(x1.min(),x1.max(),501)\n",
    "    xx, yy = np.meshgrid(grid_x, grid_y)\n",
    "    \n",
    "    # extract model predictions, using the \n",
    "    # np.c_ attribute to join together the \n",
    "    # two parts of the grid. \n",
    "    # array.ravel() converts an multidimensional\n",
    "    # array into a 1d array, and we use array.reshape()\n",
    "    # to turn the resulting predictions p \n",
    "    # back into 2d\n",
    "    \n",
    "    XX = xx.ravel()\n",
    "    YY = yy.ravel()\n",
    "    XY = np.c_[XX, YY]\n",
    "    \n",
    "    p = c.predict(XY)\n",
    "    p = p.reshape(xx.shape)\n",
    "    \n",
    "    # create the plot\n",
    "    fig, ax = plt.subplots(1)\n",
    "    \n",
    "    # use contour plot to visualize the predictions\n",
    "    ax.contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n",
    "    \n",
    "    # plot the data\n",
    "    ax.scatter(x0, x1, c = y, cmap = \"jet\", vmin = 0, vmax = 2)\n",
    "    \n",
    "    ax.set(xlabel = \"Flipper Length (mm)\", \n",
    "           ylabel = \"Culmen Depth (mm)\")\n",
    "\n",
    "# Fit the model with dropped Sex column, so that it can have 2 columns for plot regions\n",
    "clf.fit(X_train.drop(['Sex'], axis=1), y_train)\n",
    "\n",
    "# Separate the males and females, drop the sex column, predict the y's and plot\n",
    "X_male = X_train.loc[X_train['Sex'] == 1]\n",
    "X_male = X_male.drop(['Sex'], axis=1)\n",
    "y_male = clf.predict(X_male)\n",
    "\n",
    "X_female = X_train.loc[X_train['Sex'] == 2]\n",
    "X_female = X_female.drop(['Sex'], axis=1)\n",
    "y_female = clf.predict(X_female)\n",
    "\n",
    "plot_regions(clf, X_male, y_male)\n",
    "plot_regions(clf, X_female, y_female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
